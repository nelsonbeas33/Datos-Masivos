{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUNHvdCbEXMxCKHEn1+xTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonbeas33/Datos-Masivos/blob/main/tarea6/tarea6_datosMasivos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test proycto, necesitamos tener instalado spark y acceso al drive"
      ],
      "metadata": {
        "id": "Pp_QTBvpWmGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "!pip install langchain-huggingface\n",
        "\n",
        "import os\n",
        "import sys\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YylH_Q06WzZR",
        "outputId": "88ef768d-5606-4d4d-eee9-2d4d83697b02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,454 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,513 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,620 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,513 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,738 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Fetched 19.5 MB in 8s (2,443 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "57 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.26.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.19)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (3.2.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.20.3)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.1.143)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnj1hZrYWklu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType, StructType, StructField, ArrayType, StringType\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pyspark.sql.functions import size, expr, col, lower, regexp_replace, lit, trim, when\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import json\n",
        "\n",
        "# Iniciar sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Transformer Model with Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cargar archivos CSV\n",
        "#products_df = spark.read.option(\"header\", \"true\").csv(\"/opt/products_test.csv\")\n",
        "\n",
        "products_df = spark.read.option(\"header\", \"true\").csv(\"/opt/products.csv\").limit(2000)\n",
        "# Renombrar las columnas 'product_id' para evitar ambigüedad después del join\n",
        "products_df = products_df.withColumnRenamed(\"product_id\", \"product_id\")\n",
        "\n",
        "products_df.show(10)\n",
        "\n",
        "# Asegurar que las columnas 'order_product_id' y 'product_id' sean enteros\n",
        "products_df = products_df.withColumn(\"product_id\", products_df[\"product_id\"].cast(IntegerType()))\n",
        "\n",
        "# Crear la columna 'processed_product_name'\n",
        "stopwords = [\"with\", \"and\", \"in\", \"of\", \"the\", \"for\", \"a\", \"an\", \"free\", \"mix\", \"original\", \"pack\"]\n",
        "\n",
        "# Generar una expresión regular para eliminar stopwords\n",
        "stopwords_regex = r'\\b(?:' + '|'.join(stopwords) + r')\\b'\n",
        "\n",
        "# Procesar el nombre del producto\n",
        "products_df = products_df.withColumn(\n",
        "    \"processed_product_name\",\n",
        "    trim(\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                regexp_replace(\n",
        "                    regexp_replace(lower(col(\"product_name\")), r'\\d+', ''),  # Eliminar números y convertir a minúsculas\n",
        "                    r'[^a-z\\s]', ''                                         # Eliminar caracteres especiales\n",
        "                ),\n",
        "                stopwords_regex, ''                                         # Eliminar stopwords\n",
        "            ),\n",
        "            r'\\s+', ' '                                                    # Reemplazar múltiples espacios por uno solo\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Crear el vectorizador TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 1), max_features=1000)\n",
        "\n",
        "# Ajustar y transformar la lista de productos\n",
        "processed_names = products_df.select(\"processed_product_name\").rdd.flatMap(lambda x: x).collect()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_names)\n",
        "\n",
        "# Obtener los nombres de las características (palabras)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Sumar los valores de TF-IDF para cada palabra en todos los productos\n",
        "sum_tfidf = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
        "\n",
        "# Crear un diccionario de palabras con su respectiva suma de TF-IDF\n",
        "word_tfidf = {words[i]: sum_tfidf[i] for i in range(len(words))}\n",
        "\n",
        "# Ordenar las palabras por la suma de sus valores de TF-IDF, de mayor a menor\n",
        "sorted_words = sorted(word_tfidf.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Obtener las n palabras más importantes\n",
        "top_n_words_rank = sorted_words[:30]\n",
        "\n",
        "# Imprimir las palabras más importantes\n",
        "#print(\"Top n palabras más importantes:\")\n",
        "#for word, score in top_n_words_rank:\n",
        "    #print(f\"{word}: {score}\")\n",
        "\n",
        "# Ahora, aplicamos el vectorizador\n",
        "top_n_words = [word for word, score in top_n_words_rank]\n",
        "vectorizer = TfidfVectorizer(vocabulary=top_n_words)\n",
        "\n",
        "# Ajustar el vectorizador con los nombres de los productos\n",
        "vectorizer.fit(processed_names)\n",
        "\n",
        "# Crear una matriz de características de los productos\n",
        "X = vectorizer.transform(processed_names)\n",
        "\n",
        "# Convertir a una matriz densa para ver las representaciones\n",
        "dense_matrix = X.toarray()\n",
        "\n",
        "\n",
        "# Calcular la matriz de similitud coseno entre los productos, en este caos no se usa ya que kmeans no lo necesita, pero DBSCAN si\n",
        "#similarity_matrix = cosine_similarity(dense_matrix)\n",
        "\n",
        "# Número de clusters deseados (esto depende de tu caso, puedes probar diferentes valores)\n",
        "n_clusters = 20\n",
        "\n",
        "# Aplicar KMeans\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters = kmeans.fit_predict(dense_matrix)\n",
        "\n",
        "# Agrupar los productos por clúster\n",
        "clustered_products = {}\n",
        "for product, cluster_id in zip(processed_names, clusters):\n",
        "    if cluster_id not in clustered_products:\n",
        "        clustered_products[cluster_id] = []\n",
        "    clustered_products[cluster_id].append(product)\n",
        "\n",
        "#Imprimir los productos agrupados por clúster\n",
        "#for cluster_id, products in clustered_products.items():\n",
        "    #print(f\"\\nCluster {cluster_id}:\")\n",
        "    #for product in products:\n",
        "        #print(f\"  - {product}\")\n",
        "\n",
        "# Asegúrate de que mapping_list contenga solo enteros estándar (int)\n",
        "mapping_list = []\n",
        "for cluster_id, products in clustered_products.items():\n",
        "    for product in products:\n",
        "        # Asegúrate de que el virtual_product_id sea un entero estándar\n",
        "        mapping_list.append((product, int(cluster_id + 1)))  # Convertimos explícitamente a int\n",
        "\n",
        "# Especificamos el esquema explícitamente\n",
        "schema = StructType([\n",
        "    StructField(\"processed_product_name\", StringType(), True),\n",
        "    StructField(\"virtual_product_id\", IntegerType(), True)  # Especificamos que esta columna es de tipo entero\n",
        "])\n",
        "\n",
        "# Crear el DataFrame de mapeo con el esquema especificado\n",
        "mapping_cluster_df = spark.createDataFrame(mapping_list, schema)\n",
        "\n",
        "# Ahora realizar el join con el DataFrame original\n",
        "products_df = products_df.join(mapping_cluster_df, on=\"processed_product_name\", how=\"left\")\n",
        "\n",
        "# Mostrar los resultados\n",
        "products_df.show(10)\n",
        "\n",
        "order_products_df = spark.read.option(\"header\", \"true\").csv(\"/opt/order_products__prior.csv\").limit(100000)\n",
        "order_products_df_test = spark.read.option(\"header\", \"true\").csv(\"/opt/order_products__train.csv\")\n",
        "\n",
        "order_products_df = order_products_df.withColumnRenamed(\"product_id\", \"order_product_id\")\n",
        "order_products_df_test = order_products_df_test.withColumnRenamed(\"product_id\", \"order_product_id\")\n",
        "order_products_df.show(10)\n",
        "\n",
        "print(\"total de productos asignados a alguna orden:\", order_products_df.count())\n",
        "print(\"total de productos:\", products_df.count())\n",
        "\n",
        "# Asegurar que las columnas 'order_product_id' y 'product_id' sean enteros\n",
        "order_products_df = order_products_df.withColumn(\"order_product_id\", order_products_df[\"order_product_id\"].cast(IntegerType()))\n",
        "order_products_df_test = order_products_df_test.withColumn(\"order_product_id\", order_products_df_test[\"order_product_id\"].cast(IntegerType()))\n",
        "\n",
        "# Realizar un join entre los productos y las órdenes\n",
        "joined_df = order_products_df.join(products_df, order_products_df.order_product_id == products_df.product_id)\n",
        "joined_df_df_test = order_products_df_test.join(products_df, order_products_df_test.order_product_id == products_df.product_id)\n",
        "\n",
        "joined_df.show(10)\n",
        "\n",
        "# Agrupar por 'order_id' y crear las listas de productos, pasillos y departamentos\n",
        "order_grouped_df = joined_df.groupBy(\"order_id\").agg(\n",
        "    F.collect_list(\"virtual_product_id\").alias(\"virtual_product_id\"),\n",
        "    F.collect_list(\"product_id\").alias(\"product_id\"),\n",
        "    F.collect_list(\"aisle_id\").alias(\"aisle_ids\"),\n",
        "    F.collect_list(\"department_id\").alias(\"department_ids\"),\n",
        "    F.collect_list(\"processed_product_name\").alias(\"processed_product_name\")\n",
        ")\n",
        "\n",
        "# Agrupar por 'order_id' y crear las listas de productos, pasillos y departamentos\n",
        "order_grouped_df_test = joined_df_df_test.groupBy(\"order_id\").agg(\n",
        "    F.collect_list(\"virtual_product_id\").alias(\"virtual_product_id\"),\n",
        "    F.collect_list(\"product_id\").alias(\"product_id\"),\n",
        "    F.collect_list(\"aisle_id\").alias(\"aisle_ids\"),\n",
        "    F.collect_list(\"department_id\").alias(\"department_ids\"),\n",
        "    F.collect_list(\"processed_product_name\").alias(\"processed_product_name\")\n",
        ")\n",
        "\n",
        "order_grouped_df = order_grouped_df \\\n",
        "    .withColumn(\"aisle_ids\", F.expr(\"transform(aisle_ids, x -> cast(x as int))\")) \\\n",
        "    .withColumn(\"department_ids\", F.expr(\"transform(department_ids, x -> cast(x as int))\"))\n",
        "\n",
        "order_grouped_df_test = order_grouped_df_test \\\n",
        "    .withColumn(\"aisle_ids\", F.expr(\"transform(aisle_ids, x -> cast(x as int))\")) \\\n",
        "    .withColumn(\"department_ids\", F.expr(\"transform(department_ids, x -> cast(x as int))\"))\n",
        "order_grouped_df.show(10)\n",
        "\n",
        "filtered_df = order_grouped_df.filter(size(\"virtual_product_id\") > 1)\n",
        "filtered_df_test = order_grouped_df_test.filter(size(\"virtual_product_id\") > 1)\n",
        "\n",
        "# Definir la lógica para el número de elementos a eliminar\n",
        "#filtered_df = filtered_df.withColumn(\n",
        "    #\"remove_count\",\n",
        "    #F.when(size(col(\"virtual_product_id\")) <= 3, 1)  # Si el tamaño es 2 o 3, eliminar 1\n",
        "    #.otherwise(F.expr(\"cast(rand() * 2 + 1 as int)\"))  # Si es >=4, eliminar entre 1 y 2\n",
        "#)\n",
        "\n",
        "filtered_df = filtered_df.withColumn(\n",
        "    \"remove_count\",\n",
        "    F.lit(1)  # Asigna siempre el valor 1\n",
        ")\n",
        "\n",
        "filtered_df_test = filtered_df_test.withColumn(\n",
        "    \"remove_count\",\n",
        "    F.lit(1)  # Asigna siempre el valor 1\n",
        ")\n",
        "\n",
        "# Crear nuevas columnas con los elementos eliminados\n",
        "filtered_df = filtered_df \\\n",
        "    .withColumn(\"removed_order_product_ids\", expr(\"slice(virtual_product_id, size(virtual_product_id) - remove_count + 1, remove_count)\")) \\\n",
        "    .withColumn(\"removed_aisle_ids\", expr(\"slice(aisle_ids, size(aisle_ids) - remove_count + 1, remove_count)\")) \\\n",
        "    .withColumn(\"removed_department_ids\", expr(\"slice(department_ids, size(department_ids) - remove_count + 1, remove_count)\"))\n",
        "\n",
        "# Crear nuevas columnas con los elementos eliminados\n",
        "filtered_df_test = filtered_df_test \\\n",
        "    .withColumn(\"removed_order_product_ids\", expr(\"slice(virtual_product_id, size(virtual_product_id) - remove_count + 1, remove_count)\")) \\\n",
        "    .withColumn(\"removed_aisle_ids\", expr(\"slice(aisle_ids, size(aisle_ids) - remove_count + 1, remove_count)\")) \\\n",
        "    .withColumn(\"removed_department_ids\", expr(\"slice(department_ids, size(department_ids) - remove_count + 1, remove_count)\"))\n",
        "\n",
        "# Actualizar las columnas originales para conservar los elementos restantes\n",
        "filtered_df = filtered_df \\\n",
        "    .withColumn(\"virtual_product_id\", expr(\"slice(virtual_product_id, 1, size(virtual_product_id) - remove_count)\")) \\\n",
        "    .withColumn(\"aisle_ids\", expr(\"slice(aisle_ids, 1, size(aisle_ids) - remove_count)\")) \\\n",
        "    .withColumn(\"department_ids\", expr(\"slice(department_ids, 1, size(department_ids) - remove_count)\"))\n",
        "\n",
        "# Actualizar las columnas originales para conservar los elementos restantes\n",
        "filtered_df_test = filtered_df_test \\\n",
        "    .withColumn(\"virtual_product_id\", expr(\"slice(virtual_product_id, 1, size(virtual_product_id) - remove_count)\")) \\\n",
        "    .withColumn(\"aisle_ids\", expr(\"slice(aisle_ids, 1, size(aisle_ids) - remove_count)\")) \\\n",
        "    .withColumn(\"department_ids\", expr(\"slice(department_ids, 1, size(department_ids) - remove_count)\"))\n",
        "\n",
        "\n",
        "filtered_df.show(20)\n",
        "\n",
        "def create_binary_encoding(removed_product_ids, vocab_size):\n",
        "    # Crear una lista de codificaciones para cada producto retirado\n",
        "    encodings = []\n",
        "\n",
        "    # Para cada producto retirado, generar su propia codificación binaria\n",
        "    for product_id in removed_product_ids:\n",
        "        encoding = torch.zeros(vocab_size)\n",
        "        encoding[product_id] = 1  # Marcar el índice correspondiente como 1\n",
        "        encodings.append(encoding.unsqueeze(0))  # Agregar dimensión extra (1, vocab_size)\n",
        "\n",
        "    # Devolver la lista de codificaciones\n",
        "    return torch.cat(encodings, dim=0)  # Concatenar las codificaciones a lo largo del eje 0 (batch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ProductRetirementModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n",
        "        super(ProductRetirementModel, self).__init__()\n",
        "\n",
        "        # Capa de embedding para convertir los productos en vectores\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM para procesar las secuencias de productos\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Capa fully connected para predecir la probabilidad de cada producto\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Cada producto tiene una probabilidad de ser retirado\n",
        "\n",
        "        # Dropout para regularización\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Paso de embedding\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Pasamos las secuencias por el LSTM\n",
        "        rnn_out, (h, c) = self.rnn(embedded)\n",
        "\n",
        "        # Verificamos la dimensionalidad de rnn_out\n",
        "        if rnn_out.dim() == 2:\n",
        "            # Si la secuencia es de longitud 1, debemos agregar una dimensión adicional\n",
        "            rnn_out = rnn_out.unsqueeze(1)\n",
        "\n",
        "        # Usamos la salida final del LSTM\n",
        "        out = self.fc(rnn_out[:, -1, :])  # Usar la salida del último paso de la secuencia\n",
        "\n",
        "        # Aplicamos dropout\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Devolvemos las probabilidades de que cada producto esté retirado\n",
        "        return torch.sigmoid(out)  # Probabilidades entre 0 y 1\n",
        "\n",
        "# Función para entrenar el modelo\n",
        "def train_model(model, data, epochs=10, learning_rate=0.001):\n",
        "    # Optimizer y función de pérdida\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()  # Usamos BCEWithLogitsLoss porque la salida es una probabilidad entre 0 y 1\n",
        "\n",
        "    model.train()  # Establecemos el modelo en modo de entrenamiento\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for register in data:\n",
        "            ticket_product = torch.tensor(register[\"virtual_product_id\"], dtype=torch.long).unsqueeze(0)  # Hacer el batch de tamaño 1\n",
        "            removed_product = torch.tensor(register[\"removed_order_product_ids\"], dtype=torch.long)  # Hacer el batch de tamaño 1\n",
        "            # Pasamos la entrada por el modelo\n",
        "            optimizer.zero_grad()\n",
        "            #print(\"input: \", ticket_product)\n",
        "            #print(f\"Dimensiones input: {ticket_product.shape}\")\n",
        "            outputs = model(ticket_product)\n",
        "\n",
        "            #print(\"removed_order_product_ids: \", removed_product)\n",
        "            #print(\"outputs: \", outputs)\n",
        "            #print(\"removed_product: \", create_binary_encoding(removed_product, vocab_size))\n",
        "\n",
        "            loss = loss_fn(outputs, create_binary_encoding(removed_product, vocab_size))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/item_number} - \")\n",
        "        losses.append(epoch_loss/item_number)\n",
        "        accuracies.append(evaluate_model_accuracy(model, filtered_df_test.collect(), vocab_size))\n",
        "\n",
        "    print(\"perdidas\")\n",
        "    for loss in losses:\n",
        "        print(loss)\n",
        "\n",
        "    print(\"accuracies\")\n",
        "    for acc in accuracies:\n",
        "        print(acc)\n",
        "\n",
        "def evaluate_model_accuracy(model, data, vocab_size):\n",
        "    model.eval()  # Establecemos el modelo en modo de evaluación\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Desactivamos el cálculo de gradientes para la evaluación\n",
        "        for register in data:\n",
        "            ticket_product = torch.tensor(register[\"virtual_product_id\"], dtype=torch.long).unsqueeze(0)\n",
        "            removed_product = torch.tensor(register[\"removed_order_product_ids\"], dtype=torch.long)\n",
        "\n",
        "            # Pasamos la entrada por el modelo\n",
        "            outputs = model(ticket_product)\n",
        "\n",
        "            # Convertimos las salidas a probabilidades utilizando sigmoid\n",
        "            predicted_probs = torch.sigmoid(outputs)  # Esto nos da las probabilidades entre 0 y 1\n",
        "\n",
        "            # Convertimos removed_product en formato binario (0 o 1)\n",
        "            target = create_binary_encoding(removed_product, vocab_size)\n",
        "\n",
        "            # Predicción: tomar el índice con la mayor probabilidad y convertirlo en 1, el resto en 0\n",
        "            predicted = torch.zeros_like(predicted_probs, dtype=torch.int)  # Inicializar con ceros\n",
        "            max_index = torch.argmax(predicted_probs)  # Encontrar el índice con la mayor probabilidad\n",
        "            predicted[0, max_index] = 1  # Marcar como 1 el índice con la mayor probabilidad\n",
        "\n",
        "            # Calculamos las predicciones correctas\n",
        "            #print(\"predict: \", predicted)\n",
        "            #print(\"target: \", target)\n",
        "            # Encuentra el índice marcado como 1 en el target (producto que fue retirado)\n",
        "            target_idx = torch.argmax(target)  # Encuentra el índice con el valor 1 en 'target'\n",
        "\n",
        "            # Compara si el índice predicho por el modelo coincide con el índice real\n",
        "            if max_index == target_idx:\n",
        "                correct_predictions += 1  # Si el índice predicho es correcto, incrementa el contador\n",
        "                #print(\"se detecto el producto correcto: \", target_idx, \"vs\", max_index)\n",
        "\n",
        "            total_predictions += 1  # Incrementa el número total de predicciones evaluadas\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\", \" - correct: \", correct_predictions, \" de \", total_predictions)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Parámetros\n",
        "vocab_size = products_df.select(\"virtual_product_id\").distinct().count() + 1\n",
        "hidden_dim = 128  # Dimensión de la capa oculta del LSTM\n",
        "max_len = 20  # Longitud máxima de la secuencia\n",
        "epochs=10\n",
        "learning_rate=0.001\n",
        "embedding_dim = 256\n",
        "item_number = filtered_df.count()\n",
        "item_number_test = filtered_df_test.count()\n",
        "dropout_prob = 0.3\n",
        "\n",
        "print(\"vocab_size\", vocab_size)\n",
        "print(\"hidden_dim\", hidden_dim)\n",
        "print(\"max_len\", max_len)\n",
        "print(\"epochs\", epochs)\n",
        "print(\"learning_rate\", learning_rate)\n",
        "print(\"embedding_dim\", embedding_dim)\n",
        "print(\"ordenes\", item_number)\n",
        "print(\"ordenes test\", item_number_test)\n",
        "\n",
        "# Crear el modelo\n",
        "model = ProductRetirementModel(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, dropout_prob=dropout_prob)\n",
        "\n",
        "# Si tienes GPU disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Entrenar el modelo\n",
        "train_model(model, filtered_df.collect(), epochs, learning_rate)\n",
        "\n",
        "hyperparameters = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"hidden_dim\": hidden_dim,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"epochs\": epochs,\n",
        "    \"batch_size\": 1,  # Puedes agregar cualquier otro hiperparámetro relevante\n",
        "    \"dropout_prob\": 0.3\n",
        "}\n",
        "\n",
        "def save_hyperparameters(hyperparameters, filepath=\"/tmp/hyperparameters.json\"):\n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(hyperparameters, f, indent=4)\n",
        "    print(f\"Hiperparámetros guardados en {filepath}\")\n",
        "\n",
        "torch.save(model, \"/tmp/product_retirement_full_model.pth\")\n",
        "\n",
        "np.save(\"/tmp/dense_matrix.npy\", dense_matrix)\n",
        "np.save(\"/tmp/clusters.npy\", clusters)\n"
      ]
    }
  ]
}