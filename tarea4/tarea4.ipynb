{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzBoTcHgcAA0iRMgneeWsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonbeas33/Datos-Masivos/blob/main/tarea4/tarea4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este es una prueba de concepto de lo aprendido en clase sobre el uso de spark para el proyecto"
      ],
      "metadata": {
        "id": "TTFRw3btOh9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4jHwMPZOhS5"
      },
      "outputs": [],
      "source": [
        "#docker\n",
        "\n",
        "# Usar una imagen base de Bitnami Spark\n",
        "FROM bitnami/spark:latest\n",
        "\n",
        "# Cambiar a usuario root para instalar paquetes\n",
        "USER root\n",
        "\n",
        "# Actualizar el sistema e instalar Python y pip\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    python3 \\\n",
        "    python3-pip \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# Instalar PyTorch y sus dependencias\n",
        "RUN pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "#falta instalar panda\n",
        "pip install pandas\n",
        "\n",
        "# Establecer el directorio de trabajo\n",
        "WORKDIR /app\n",
        "\n",
        "# Comando por defecto para iniciar el contenedor como maestro de Spark\n",
        "CMD [\"/opt/bitnami/spark/bin/spark-class\", \"org.apache.spark.deploy.master.Master\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usaremos spark para realizar el preprocesamiento de los datos del proyecto, se pretende generar una agrupación por orden de compra"
      ],
      "metadata": {
        "id": "aXvrDaw3O0YZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import collect_list\n",
        "from pyspark.sql.types import IntegerType\n",
        "import random\n",
        "from pyspark.sql import functions as F\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "# Iniciar sesión de Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Transformer Model with Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Cargar archivos CSV\n",
        "order_products_df = spark.read.option(\"header\", \"true\").csv(\"/opt/order_products__prior.csv\")\n",
        "products_df = spark.read.option(\"header\", \"true\").csv(\"/opt/products.csv\")\n",
        "\n",
        "# Renombrar las columnas 'product_id' para evitar ambigüedad después del join\n",
        "order_products_df = order_products_df.withColumnRenamed(\"product_id\", \"order_product_id\").limit(100)\n",
        "products_df = products_df.withColumnRenamed(\"product_id\", \"product_id\")\n",
        "\n",
        "# Asegurar que las columnas 'order_product_id' y 'product_id' sean enteros\n",
        "order_products_df = order_products_df.withColumn(\"order_product_id\", order_products_df[\"order_product_id\"].cast(IntegerType()))\n",
        "products_df = products_df.withColumn(\"product_id\", products_df[\"product_id\"].cast(IntegerType()))\n",
        "\n",
        "# Mostrar las primeras filas de ambos DataFrames para ver su estructura\n",
        "order_products_df.show(5)\n",
        "products_df.show(5)\n",
        "\n",
        "# Realizar un join entre los productos y las órdenes\n",
        "joined_df = order_products_df.join(products_df, order_products_df.order_product_id == products_df.product_id)\n",
        "# Mostrar el resultado del join para verificar\n",
        "joined_df.show(5)\n",
        "\n",
        "# Agrupar por 'order_id' y crear las listas de productos, pasillos y departamentos\n",
        "order_grouped_df = joined_df.groupBy(\"order_id\").agg(\n",
        "    F.collect_list(\"order_product_id\").alias(\"product_ids\"),\n",
        "    F.collect_list(\"aisle_id\").alias(\"aisle_ids\"),\n",
        "    F.collect_list(\"department_id\").alias(\"department_ids\")\n",
        ")\n",
        "\n",
        "order_grouped_df = order_grouped_df \\\n",
        "    .withColumn(\"aisle_ids\", F.expr(\"transform(aisle_ids, x -> cast(x as int))\")) \\\n",
        "    .withColumn(\"department_ids\", F.expr(\"transform(department_ids, x -> cast(x as int))\"))\n",
        "\n",
        "\n",
        "order_grouped_df.show(30)\n",
        "\n",
        "# Calcular el máximo valor de los IDs en cada columna y convertir el resultado a entero\n",
        "max_product_id = order_grouped_df.selectExpr(\"explode(product_ids) as product_id\") \\\n",
        "    .agg(F.max(\"product_id\").alias(\"max_product_id\")) \\\n",
        "    .collect()[0][\"max_product_id\"]\n",
        "\n",
        "# Calcular el máximo de cada columna después de la conversión\n",
        "max_aisle_id = order_grouped_df.selectExpr(\"explode(aisle_ids) as aisle_id\") \\\n",
        "    .agg(F.max(\"aisle_id\").alias(\"max_aisle_id\")) \\\n",
        "    .collect()[0][\"max_aisle_id\"]\n",
        "\n",
        "max_department_id = order_grouped_df.selectExpr(\"explode(department_ids) as department_id\") \\\n",
        "    .agg(F.max(\"department_id\").alias(\"max_department_id\")) \\\n",
        "    .collect()[0][\"max_department_id\"]\n",
        "\n",
        "\n",
        "print(f\"Máximo índice de max_product_id: {max_product_id}\")\n",
        "print(f\"Máximo índice de aisle_ids: {max_aisle_id}\")\n",
        "print(f\"Máximo índice de max_department_id: {max_department_id}\")\n",
        "\n",
        "order_grouped_df.printSchema()\n",
        "\n",
        "\n",
        "\n",
        "# Asegurarse de que los valores sean enteros y calcular el tamaño del vocabulario\n",
        "product_vocab_size = int(max_product_id) + 1  # +1 para padding\n",
        "aisle_vocab_size = int(max_aisle_id) + 1  # +1 para padding\n",
        "department_vocab_size = int(max_department_id) + 1  # +1 para padding\n",
        "\n",
        "#parametros del modelo\n",
        "embed_dim = 64\n",
        "\n",
        "print(f\"Vocabulario de Productos: {product_vocab_size}\")\n",
        "print(f\"Vocabulario de Pasillos: {aisle_vocab_size}\")\n",
        "print(f\"Vocabulario de Departamentos: {department_vocab_size}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yCp0TUkCOvW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}